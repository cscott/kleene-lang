
\documentclass[letterpaper,12pt]{article}
% \documentclass[a4paper,12pt]{article}
% twocolumn letterpaper 10pt 11pt twoside

% for other type sizes, 8, 9, 10, 11, 12, 14pt, 17pt, 20pt
% \documentclass[14pt]{extarticle}
% also extbook, extletter available
% \usepackage{extsizes}

%\usepackage{endnotes}
% then put \theendnotes where you want them

\usepackage{times}
\usepackage{xspace}


%\usepackage{alltt}
\usepackage{fancyvrb}  % \begin{Verbatim}[fontsize=\small]
% or [fontsize=\footnotesize]
%\usepackage{upquote}
% affects \verb and verbatim
% to get straight quotes, straight single quote, straight double
% quotes in verbatim environments


%\usepackage{latexsym}  % \LaTeX{} for LaTeX;  \LaTeXe{} for LaTeX2e
%\usepackage{mflogo}    % \MF{}  for METAFONT;  \MP for METAPOST
%\usepackage{url}       % \url{http://www.xrce.xerox.com/people/beesley}
%\usepackage{lscape}    % allows \begin{landscape} ... \end{landscape}

%\usepackage{tipa}
%\include{ipamacros}  % my macros to allow same input for DA and IPA
%\usepackage{desalph}
%\usepackage{arabtex} % see usepackage{buck} and setcode{buck} below
%\usepackage{buck}
%\usepackage{mxedruli}

%\usepackage{epsfig}
%\usepackage{pslatex}  % make whole doc. use postscript fonts

% parallel columns, see also multicol
%\usepackage{parcolumns}
%...
%\begin{parcolumns}[<options>]{3}
%\colchunk{ column 1 text }
%\colchunk{ column 2 text }
%\colchunk{ column 3 text }
%\colplacechunks
%...
%\end{parcolumns}


% for more of these names, see Guide to LaTeX, p. 351
%\providecommand*{\abstractname}{}     % in case the style defines one
%\renewcommand*{\abstractname}{Transcriber notes}
%\renewcommand*{\figurename}{Figure}
%\renewcommand*{\tablename}{Table}
%\renewcommand*{\bibname}{Bibliography}
%\renewcommand*{\refname}{References}

\providecommand{\acro}{}\renewcommand{\acro}{\textsc}
\providecommand{\defin}{}\renewcommand{\defin}{\textsc}

\newcommand{\xmlelmt}{\texttt}
\newcommand{\xmlattr}{\texttt}
\newcommand{\key}{\textbf}
\newcommand{\translit}{\texttt}

% forced pagebreak
%\newpage

%\usepackage{ulem}
%    \uline{important}   underlined text
%    \uuline{urgent}     double-underlined text
%    \uwave{boat}        wavy underline
%    \sout{wrong}        line drawn through word (cross out, strike out)
%    \xout{removed}      marked over with //////.
%    {\em phasized\/}  | In LaTeX, by default, these are underlined; use
%    \emph{asized}     | \normalem or [normalem] to restore italics
%    \useunder{\uwave}{\bfseries}{\textbf}
%                        use wavy underline in place of bold face


%                        \usepackage{natbib}
%\usepackage[authoryear]{natbib}
% compatible with \bibliographystyle{plain}, harvard, apalike, chicago, astron, authordate

%\citet for "textual"   \citet{jon90} ->  Jones et al. (1990)
%\citet[before][after]{key} e.g. \citet[see][p.~47]{jon90} --> 
%         see Jones et al.(1990, chap. 2)
%\citet[chap. 2]{jon90}	    -->    	Jones et al. (1990, chap. 2)
%\citet[after]{key}

%   citep for "parenthetical"
%\citep{jon90}	    -->    	(Jones et al., 1990)
%\citep[chap. 2]{jon90}	    -->    	(Jones et al., 1990, chap. 2)
%\citep[see][]{jon90}	    -->    	(see Jones et al., 1990)
%\citep[see][chap. 2]{jon90}	    -->    	(see Jones et al., 1990, chap. 2)

%\citep for "parenthetical" (author's name in parens)
%\citep  similar
%
%\citet*{key}  list all authors, not just et.al
%\citetext{priv.\ comm.} comes out as (priv. comm.)
%
%just the author or year
%\citeauthor{key} comes out as "Jones et al."
%\citeauthor*{key} comes out as "Jones, Sacco and Vanzetti"
%\citeyear{key}   comes out as 1990
%\citeyearpar{key}            (1990)
%
%Rare stuff:
%use \Citet and \Citep for exceptional forcing of initcap on names
%like 'della Robbia' when it appears first in a sentence.
%
%\citealt like \citet but without parens
%\citealp like \citep but without parens
%


% fancyheadings from The Book (old, obsolete, I think)
%\usepackage{fancyheadings}
%\pagestyle{fancyplain}
% remember the chapter title
%\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
%\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
%\lhead[\fancyplain{}{\small\scshape\thepage}]{\fancyplain{}{\small\scshape\rightmark}}
%\rhead[\fancyplain{}{\small\scshape\leftmark}]{\fancyplain{}{\small\scshape\thepage}}
%\cfoot{}

% new fancyhdr package
%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\fancyhead{}

%% L/C/R denote left/center/right header (or footer) elements
%% E/O denote even/odd pages

%% \leftmark, \rightmark are chapter/section headings generated by the 
%% book document class

%\fancyhead[LE,RO]{\slshape\thepage}
%\fancyhead[RE]{\slshape \leftmark}
%\fancyhead[LO]{\slshape \rightmark}
%\fancyfoot[LO,LE]{\slshape Short Course on Asymptotics}
%\fancyfoot[C]{}
%\fancyfoot[RO,RE]{\slshape 7/15/2002}

% another example
%\fancyhead[LE]{\thepage}
%\fancyhead[CE]{\bfseries Beesley}
%\fancyfoot[CE]{First Draft}
%\fancyhead[CO]{\bfseries My Article Title}
%\fancyhead[RO]{\thepage}
%\fancyfoot[CO]{For Review and Editing Only}
%\renewcommand{\footrulewidth}{0.4pt}

% \vspace{.5cm}
% c, l, r, p{1cm}
%\begin{tabular}{}
%\hline
%   &  &  &   \\
%\hline
%\end{tabular}
% \vspace{.5cm}


% bigbox -- puts a box around a float
% for {figure}, {table} or {center}

\newdimen\boxfigwidth  % width of figure box

\def\bigbox{\begingroup
  % Figure out how wide to set the box in
  \boxfigwidth=\hsize
  \advance\boxfigwidth by -2\fboxrule
  \advance\boxfigwidth by -2\fboxsep
  \setbox4=\vbox\bgroup\hsize\boxfigwidth
  % Make an invisible hrule so that
  % the box is exactly this wide
  \hrule height0pt width\boxfigwidth\smallskip%
% Some environments like TABBING and other LIST environments
% use this measure of line size -
% \LINEWIDTH=\HSIZE-\LEFTMARGIN-\RIGHTMARGIN?
  \linewidth=\boxfigwidth
}
\def\endbigbox{\smallskip\egroup\fbox{\box4}\endgroup}


% example
% \begin{figure}
%   \begin{bigbox}
%     \begin{whatever}...\end{whatever}
%     \caption{}
%     \label{}
%   \end{bigbox}
% \end{figure}
% 
% N.B. put the caption and label inside the bigbox

%\usepackage{graphicx}
% Sample Graphics inclusion; needs graphicx package
%\begin{figure}[ht]
%\begin{bigbox}
%\centering
%\includegraphics{foobar.pdf}   # e.g. PNG, PDF or JPG, _not_ EPS
%\caption{}
%\label{lab:XXX}
%\end{bigbox}
%\end{figure}

%\pagestyle{empty}  % to suppress page numbering

% turn text upside down
%\reflectbox{\textipa{\textlhookp}}
% prevent line break:   \mbox{...}

\hyphenation{hy-po-cri-tical ri-bald}

%%%%%%%%%%%%%%%%%%%%  title %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Tokenization for `test' in Kleene/OpenFst:\\
Notes to Myself}
\author{Kenneth R.~Beesley}

% to override automatic "today" date
\date{15 October 2009}

%\usepackage{makeidx}
%\makeindex
% see \printindex below in the document
%\usepackage{showidx}   % print proofs showing indexed locations!!!

%%%%%%%%%%%%%%%%%%%%%% document %%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

\begin{abstract}
This document explains how tokenization-for-test (aka tokenization
for lookup/lookdown, or tokenization for `apply up' and `apply
	down') is done in the Kleene GUI.  This solution appears to be
	working well enough
	for manual testing, where performance is not critical, but it might not be attractive at all for any production
		environment.

The almost unspoken assumption behind all this work is that we
want, for the `test' facility, to emulate the behavior found in the
Xerox `apply up' and `apply down' commands.  An appendix discusses
an alternative.
\end{abstract}

\section{The Kleene GUI `test' Feature}

In the Kleene \acro{gui}, one can invoke

\begin{Verbatim}[fontsize=\small]
test $yourFst ;
\end{Verbatim}

\noindent
to bring up a `test' window.  The user can then type in strings to
be used for either generation (lookdown) or analysis (lookup).  As
these strings may contain multichar symbols, they need to be
``tokenized for test''.

\section{Mindtuning for OpenFst Labels}

The labels in OpenFst networks are always two-level, i:o, visualized in
the AT\&T tradition as having an ``input'' side and an ``output'' side.
From a purely mechanical point of view, the ``input'' side corresponds to
the Xerox ``upper'' side, and the ``output'' side corresponds to the
Xerox ``lower'' side.

The
actual labels that appear in networks are ints.  By default, in the
default Tropical Semiring, supplied by OpenFst, the ints are
machine-native ints, usually 32 bits.\footnote{In
OpenFst, the networks are templated on the Arc type, which effectively
defines the semiring.  Users can define their own Arc types, which might
include limiting the size of the labels; for example, one might define the
labels to be short ints.}  Using plain OpenFst off the shelf, one must define
manually the mapping between symbols like `a', `b', `c' and the integers
used to represent them in labels; and the mapping can be different on the input side
and the output side.

In Kleene/OpenFst networks,

\begin{enumerate}
\item
The integer used to represent each character like `a' is the official
Unicode code point value.  
\item
The encoding of a symbol like `a' is always the same on the input and output
sides.
\item
Supplementary Unicode characters are also stored as their official Unicode
code point value.  (All Unicode code point values can be encoded in 21
bits.)
\item
Kleene multichar symbols are stored using an arbitrary value from the
Public Use Areas (PUA) in the supplementary area (the areas known as
Planes 15 and 16).
\end{enumerate}

\section{Mindtuning for Kleene `test' Tokenization}

Some high-level points:

\begin{enumerate}
\item
The tokenization in the `test' facility is currently performed using ICU4J (ICU for
Java)
Transliterator objects.
\item
The name Transliterator is something of a misnomer.  They can be used for
what linguists call transliteration, e.g.\@ taking a Russian String in
Cyrillic characters and producing a transliteration into Roman character,
but they can do much more.  Some of the
documentation refers more generally to String ``Transforms'' that can be
performed using objects of the Transliterator class.
\item
In the `test' facility there are separate transliterators used for generation vs.\@ analysis.  
\item
The semantics of Transliterator operation is not obvious, and perhaps not
always well document.  In general
\begin{itemize}
\item
A Transliterator has a match pointer that moves from beginning to end
through a String.
\item
At each point during the pass through the String, the Tokenizer tries to
apply its set of ``rules'' in the order in which the rules were defined.
The first rule (if any) that matches is applied, and the remaining rules in the set
are ignored.  If you want longer matches to have precedence over shorter
matches, then in the definition of the Tokenizer, the rules for longer strings have to be ordered before the rules
for shorter strings.
\item
Each Transliterator rule can explicitly specify where the match pointer is relocated after the match, but
by default it is located just after the matched
substring/replacement.

\end{itemize}
\item
The way that these Transliterators are created is non-trivial. For the
Kleene `test' function, the Transliterators are created from a set of rules
written in a String.  Simple match-and-replace rules might look like

\begin{Verbatim}[fontsize=\small]
ph > f ;
t > \u062A ;
\end{Verbatim}

\noindent
Such rules can be handwritten; in Kleene the String containing the rules is generated
programmatically and then passed to the
Transliterator constructor.\footnote{The rules are defined in a String,
which is passed to the constructor, but I think it highly unlikely that the
rules are kept and searched in a simple String format at runtime.}

\end{enumerate}

\section{Pieces of Code}

The code involved in tokenization for `test' includes four principal
classes and methods:

\begin{enumerate}
\item
TestFstInternalFrame.java (defines the GUI widget used for testing; uses
the Transliterators)
\item
iterate4mcs()  (a ``native'' C++ function, in kleeneopenfst.cc, that traverses the network,
finding the multichar symbols that actually appear on upper and lower
labels, respectively)
\item
OpenFstInterpreterKleeneVisitor.java (search for the ASTtest\_statement method)
\item
TranslitTokenizerBuilder.java (creates the Transliterator objects)

\end{enumerate}

\subsection{TestFstInternalFrame.java}

This class defines the GUI widget that allows the user to type in either
an input/upper string or an output/lower string for testing.  Because
these strings may contain multichar symbols, they may need to be
tokenized.  E.g. if the user has defined \texttt{[Noun]}, \texttt{[Pl]}
and \texttt{[Sg]} as multichar symbols, and the user types in the string

\begin{Verbatim}[fontsize=\small]
dog[Noun][Pl]
\end{Verbatim}

\noindent
for generation, then the input string needs to be tokenized into 

\begin{Verbatim}[fontsize=\small]
d
o
g
[Noun]
[Pl]
\end{Verbatim}

\noindent
Longer matches should have precedence over shorter matches, so if the
user has been unwise enough to define two multichar symbols with names
\texttt{aaa} and \texttt{aa}, then the longer one should win.

\subsection{iterate4mcs()}

This C++ function, found in kleeneopenfst.cc, is called from Java to
traverse the network (which is a C++ object) and find the multichar
symbols that actually appear in labels.  It distinguishes between labels
found on the input side and those found on the output side.

This function uses the standard OpenFst iterators for iterating through
all the states in the network, and for each state, to iterate through the
exit arcs.

\subsection{OpenFstInterpreterKleeneVisitor.java}

When the user invokes

\begin{Verbatim}[fontsize=\small]
test $myFst ;
\end{Verbatim}

\noindent
the syntax is parsed as a test\_statement, and it is interpreted by the
ASTtest\_statement method in OpenFstInterpreterKleeneVisitor.java.  This
method creates an instance of TranslitTokenizerBuilder.java, which has two key methods

\begin{enumerate}
\item
.registerMcsInput(), which is called to register a multichar symbol found
on the input (upper) side
\item
.registerMcsOutput(), which is called to register a multichar symbols found
on the output (lower) side
\end{enumerate}

The ASTtest\_statement method then calls the native (C++) function
iterate4mcs(), passing the Fst to be traversed, the handle to the
TranslitTokenizerBuilder, and the starting (lowest) value used to encode
multichar symbols.\footnote{Currently in Kleene, multichar symbols are stored in
Planes 15 and 16 in the supplementary area.  Plane 15 starts at U+F0000.}
As the C++ function iterate4mcs() iterates through the states and arcs of
the Fst, it makes callback to .registerMcsInput() and .registerMcsOutput(),
respectively, as appropriate.  The TranslitTokenizerBuilder collects the set
of input-side multichar-symbol labels, and the set of output-side multichar-symbol
labels.

After iterate4mcs() returns, the ASTtest\_statement method can call methods
in the TranslitTokenizerBuilder to get back the Transliterators needed for
testing.  

The method then creates an instance of TestFstInternalFrame, passing the
sigma of the Fst and the 
Transliterators to the constructor.

\subsection{TranslitTokenizerBuilder.java}

\subsubsection{Collecting/Finding the Multichar Symbols}

This is the class that knows how to build the Tokenizers, once it knows

\begin{enumerate}
\item
The set of multichar symbols (the int values) that appear in the Sigma---stored in
a HashSet$<$Integer$>$ called multicharCpvSigma.
\item
The set of multichar symbols (the int values) that actually appear on input-side
labels---stored in a HashSet$<$Integer$>$ called multicharCpvInput.
\item
The set of multichar symbols (the int values) that actually appear on output-side
labels---stored in a HashSet$<$Integer$>$ called multicharCpvOutput.
\end{enumerate}

\noindent
This class iterates through the Sigma itself to find the full set of
multichar symbols known to the network.  The native function iterate4mcs(),
which traverses the actual network using the usual OpenFst C++ idioms, has
a handle to the TranslitTokenizerBuilder and makes call-backs to the
.registerMcsInput() and .registerMcsOutput() methods of the
TranslitTokenizerBuilder, which collects the integer values in
multicharCpvInput and multicharCpvOutput, respectively.

The multichar-symbol labels that appear only on the input side are
(multicharCpvInput - multicharCpvOutput).  Similarly, the multichar-symbol
labels that appear only on the output side are (multicharCpvOutput -
multicharCpvInput).

The tokenizer for generation (where the user-typed string is matched
against the upper/input side of the network) is based on all the multichar
symbols that appear in the Sigma, minus the multichar symbols that appear
only in output-side labels, i.e.

\begin{Verbatim}[fontsize=\small]
multicharCpvSigma - (multicharCpvOutput - multicharCpvInput)
\end{Verbatim}

\noindent
And the tokenizer for analysis (where the user-typed string is matched
against the lower/output side of the network) is based on all the multichar
symbols that appear in the Sigma, minus the multichar symbols that appear
only in input-side labels, i.e.

\begin{Verbatim}[fontsize=\small]
multicharCpvSigma - (multicharCpvInput - multicharCpvOutput)
\end{Verbatim}

\subsubsection{Creating the Transliterators}

TranslitTokenizerBuilder has a method .getTranslitTokenizer(boolean
for\_input\_side) that is called twice from the interpreter (ASTtest\_statement
	method) to get the input-side Transliterator and (with a
	\texttt{false} argument) the output-side Transliterator.  

TranslitTokenizerBuilder has access to the symmap, a single object of class
SymMap.java that maintains, for any single Kleene session, the mappings
from symbol names (Strings) to int, and from int to symbol Strings.  For a
given label (integer) i, the String name is symmap.getsym(i).  For a given
String name s, the corresponding integer value is symmap.getint(s).

TranslitTokenizerBuilder gets the set of code-point-value ints that need to
be tokenized for the indicated side (these ints are in the supplementary
character range), looks up the string names using symmap.getsym(i), and
sorts the $<$String, int$>$ pairs by String length, using a TreeMap with a longest-to-shortest
Comparator.  The TreeMapped/sorted pairs are then retrieved as a Set, and
the code iterates through the set.  Here's the code to build a StringBuffer
of Transliterator ``rules''.

\begin{Verbatim}[fontsize=\footnotesize]
StringBuilder rulebuf = new StringBuilder() ;
rulebuf.append("use variable range 0xF000 0xF4FF; ") ;
// Undocumented feature of Transliterator:  it uses some of the PUA,
// by default _ALL_ of the PUA (0xF000 through 0xF8FF), for its own
// internal variables, preventing my use of that area; but you can 
// tell Transliterator to limit itself to a subportion, e.g. 0xF000
// to 0xF4FF, as shown above, leaving me free to use the rest (Kleene
// limits itself to Plane 15 and up

// Now loop through the entries; need to create a String of "transliteration"
// rules of the form
// inputchars > outputchars
// for all multichar symbols.
// To avoid problems with special characters, both input and output will
// be expressed in the rule-string as sequences of backslash-uHHHH sequences.
// The mapping to backslash-uHHHH sequences is done by tr2JavaEsc, which
// is a Transliterator of type Any-Hex/Java, which is supplied by ICU.  It
// maps any Java String to a sequence of backslash-uHHHH sequences.


String key ;
int val ;
// loop through the entries in the (sorted) set 
for (Map.Entry<String, Integer> me: set) {
    key = me.getKey() ;
    val = me.getValue().intValue() ;  // a value in the surrogate range
    // not representable in one 16-bit code unit
    if (key.length() > 1) {
        rulebuf.append(tr2JavaEsc.transliterate(key) + " > " 
            + tr2JavaEsc.transliterate(stringFromCpv(val)) + " ; ") ;
    } else {
        // shouldn't they all be multichar symbols, screened already?
        System.out.println("See TranslitTokenizerBuilder, problem XXX") ;
    }
}

// Creates a rulebuf that looks like this (for [Noun], [Sg],
// [Pl]) with linebreaks inserted for readability
// use variable range 0xF000 0xF4FF; 
//    \u005B\u004E\u006F\u0075\u006E\u005D > \uDB80\uDC03 ; 
//    \u005B\u0053\u0067\u005D > \uDB80\uDC04 ; 
//    \u005B\u0050\u006C\u005D > \uDB80\uDC05 ; 
//
// Note that the output surrogate values get represented as the
// sequence of a high and a low surrogate, e.g.
// \uDB80\uDC03 represents 0xF0003,
// because the result is still a Java String, which means UTF-16.
\end{Verbatim}

\noindent
It took me a while to get this all to work, partly because ICU, like most other
applications, provides less than adequate documentation for dealing with
supplementary characters.

Once a StringBuffer of such rules is available, it can be used to instantiate a
new Transliterator ``from rules'', thus:

\begin{Verbatim}[fontsize=\footnotesize]
Transliterator trTok = Transliterator.createFromRules(trID, 
                                        rulebuf.toString(), 
                                  Transliterator.FORWARD) ;
\end{Verbatim}

\section{From Transliterated String to int[] of Code Point Values}

The Strings resulting from the input or output tokenization can be viewed
(abstractly) as consisting of a sequence of code point values, though the
supplementary code point values (representing Unicode-defined supplementary
characters, and Kleene-user-defined multichar symbols) are still represented
using two 16-bit code units.

The next trick is to take the Tokenized String and get an array of int, wherein
each element of the int array is a code point value.  This is done in
TestFstInternalFrame using a UCharacterIterator, also from ICU4J.

\begin{Verbatim}[fontsize=\footnotesize]
// UCharacterIterator
// knows how to iterate through the code point values of
// a Java String (not the 16-bit code units, but the 
// Unicode code point values!)
UCharacterIterator iter = UCharacterIterator.getInstance(cpvstr) ;

int codepoint ;
int index = 0 ;
while ((codepoint = iter.nextCodePoint()) != UCharacterIterator.DONE) {
    // the BMP chars typed by the user might not yet
    // be in the symmap (any multichar symbols should
    // already be in the symmap, else they couldn't
    // have been recognized by the tokenizer)
    if (Character.charCount(codepoint) == 1) {  // if BMP
        symmap.putsym(String.valueOf((char) codepoint));
    }

    intArray[index++] = codepoint ;
}
\end{Verbatim}

This int[] can then be passed to a native (C++) function that builds a one-path
Fst, with one int labeling each arc.  The resulting one-path Fst is then composed
with the Fst being tested, on the appropriate side, and the output is read off the opposite side.

\appendix

\section{Kempe's Alternative}

It must be said that the tokenization-for-test used in the Xerox `apply up'
and `apply down' commands causes a fair amount of confusion for users,
especially if they are sloppy in their definition of multichar symbols.
Users are warned \emph{not} to create multichar symbols that look like normal
strings of alphabetic letters, e.g. \texttt{ing}, but the unfortunate
\texttt{xfst} syntax often leads users to do this anyway.  Users are
urged to define multichar symbols that look like \texttt{[Noun]} and
\texttt{[Pl]}, with distinctive delimiting punctuation, but some of them
don't.  Bad multichar symbols can lead to unexpected tokenization,
causing strings that look correct to fail to match.

In his own HtFst language, also based on OpenFst, Andr\'e Kempe has
apparently opted to avoid this whole problem.  For one thing, he may not
have a truly interactive `test' facility at all.  For another, when the user
types input for testing, Kempe requires that the input be a valid \emph{regular
expression}, not just a flat string.  Thus if \texttt{[Noun]} and {[Pl]}
were multichar symbols, Kempe would require his users to type the valid
regular expression (in HtFst syntax)

\begin{Verbatim}[fontsize=\small]
dog`[Noun]'`[Pl]'
\end{Verbatim}

\noindent
Effectively, Kempe requires the user to explicitly mark the multichar
symbols manually.

In Kempe's system, he also assumes/requires that ``surface'' strings,
i.e.\@ strings that occur in real text, do not contain multichar symbols
and so do not require a tokenization step when tested or looked up in
production environments.

\end{document}
