\chapter{Examples with Weights}

\label{chapt:exampleswithweights}

\section{Introduction}

This chapter, definitely work in progress, presents examples that
use weights.  New examples will be added as they become available.

\section{Weighted Edit Distance for Spelling Correction}

The following script, based on an example by M\r{a}ns Huld√©n, illustrates how weighted \fst{}s
can be used to make a spelling corrector that returns a set of possible
corrections ranked by likelihood, or even just the best (most likely)
correction, for a misspelled word.

The first phenomenon that we need to model is the degree of dissimilarity between an
observed misspelled word and a correctly spelled possible correction.  For example, the misspelled
word \emph{accomodation} is intuitively very similar to the correct word
\emph{accommodation}, differing in only one missing letter.  Similarly,
\emph{panick}
is very similar to \emph{panic}, differing in having one extra letter.  And
\emph{advertize} is similar to the correctly spelled \emph{advertise}, differing only
in one changed letter. Increasing in dissimilarity, \emph{camoflague} differs from
\emph{camouflage} in two letters, \emph{garentee} also differs from
\emph{guarantee} in
two letters, and \emph{bizness} differs from \emph{business} in three letters.

We can quantify the amount of dissimilarity between two words as the \emph{edit distance},
a count of the number of editing changes, or, perhaps a more sophisticated measure
of the amount of work, required to change one word into the
other.  The changes could include adding a letter, deleting a letter, or changing
one letter into another letter.

Weighted Kleene \fst{}s have weights that are ``costs,'' under the Tropical Semiring, and
the edit distance can be formalized in Kleene as the cost of
transducing one word into another, assigning a cost to deleting a
letter, a cost to adding a letter and a cost to changing one letter into
another letter.  In the Tropical Semiring, the costs associated with multiple changes are simply
added together to yield the total cost.  Mapping a letter to itself is free, involving no cost,
i.e., a neutral cost of 0.0.  Of
course, with
enough editing changes, any word can be changed to any other word,
but intuitively we want to concentrate on the possible corrections that
involve the lowest cost, the fewest edits.  These intuitions will be encoded in what is known 
as a \emph{error model} or \emph{edit model}.

Another phenomenon that needs to be modeled is the fact that some words are
simply more common than others in any reasonably sized corpus.  Looking at a misspelled word
in isolation,
if a set of possible corrections having the same edit distance includes a more common word and
	some less
common words, the more common word is more likely to be the best correction.  For
example, the misspelled \emph{frm} differs in only one letter from the possible
corrections \emph{farm}, \emph{firm} and \emph{from}, but \emph{from} is far more common than
the alternatives and is---considered in isolation---more likely to be the intended
word.  These intuitions will be encoded in a unigram \emph{language model}.

We will build our weighted edit-modeling transducer from four main parts.  The first
part is the identity mapping:

\begin{Verbatim}
$ident = . <0.0> ;	// map any symbol to itself
\end{Verbatim}

\noindent
Recall that . (the dot, period or full stop) denotes any symbol or the transducer that maps any
symbol to itself.  Such identity mappings are free, having no cost in
our spell-checking example, so we assign the identity mapping a weight of 0:0.

The second part is

\begin{Verbatim}
$changecost = <7.0> ;
$delete = .:"" $changecost ;  // map a symbol 
                              // to the empty string
\end{Verbatim}

\noindent which denotes the transducer that maps any symbol, denoted as . (dot),
downward to the empty string, denoted \texttt{""}.  We arbitrarily assign the
weight of 7.0 to such a deletion, a value that we can adjust later.

The third part is

\begin{Verbatim}
$insert = "":. $changecost ;  // map the empty string 
                              // to a symbol
\end{Verbatim}

\noindent which denotes the transducer that maps the empty string downward to any
symbol, inserting a symbol where there was none before.  Such mappings are technically known as
epentheses.  We assign such
insertions the weight of 7.0.

Finally, the fourth part is

\begin{Verbatim}
$swap = ( .:. - . ) $changecost ;
\end{Verbatim}

\noindent
Recall that \texttt{.:.} denotes the transducer that maps any symbol to any
symbol, including itself.  Then \texttt{(~.:.~- .~)} denotes the transducer
that maps any symbol to any other symbol, so \emph{not} including identity mappings.  We
assign such letter swaps the cost of 7.0.

Armed with these definitions, our first draft of the model for transducing between a 
misspelled word and a properly spelled word, the \emph{edit model}, is a
sequence of zero or more identity mappings, deletions, insertions or swaps,
in Kleene terms:

\begin{Verbatim}
$editModel = ( $ident | $delete | $insert | $swap )* ;
\end{Verbatim}

We also need a unigram \emph{language model} that encodes the language (i.e., the
set) of properly spelled words that will serve as possible corrections.  
Initially, such a model can be constructed
as a simple unweighted union of words,

\begin{Verbatim}
$langModel = a | back | bake | bee | carrot | deer | eye ;
\end{Verbatim}

\noindent
extending it eventually to hundreds of thousands or even millions of words.  For the purposes
of this example, we can simply search the web for posted wordlists, such as the
``Simpsons Frequency Dictionary,''\footnote{\url{http://pastebin.com/anKcMdvk}} which contains the 5000 most frequent words
found in open subtitles from \emph{The Simpsons} television series.  If we download this list, we can, with a
little editing,\footnote{There are some apparently extraneous punctuation letters
	in this list, and for Kleene regular expressions, some vertical bars (``pipes''), digits and punctuation
letters need to be removed or literalized using double quotes or backslashes.} convert it into a useful test model.

\begin{Verbatim}
$langModel = the | you | i | a | to | and | of | it  ... ;
// the full list has 5000 words, some needing editing 
//     to compile correctly in Kleene
\end{Verbatim}

\noindent
Then if the misspelled word is \emph{frm}, the language of possible corrections,
\verb!$corr!, is computed as

\begin{Verbatim}
$corr = frm _o_ $editModel _o_ $langModel ;
// FstType: vector, Semiring: standard, 12992 states,
56693 arcs, 2330246 paths, Transducer, Weighted, Closed Sigma
\end{Verbatim}

\noindent
When I run this experiment, using the bare Simpsons wordlist, with my edits, Kleene
informs me that the result \texttt{\$corr} contains 2330246 paths, representing
the fact that, with enough edits, \emph{frm} can be transduced into any of the
5000 words in the language model, in multiple ways.

At this point, we can start to use the weights (costs) to focus on the more
likely corrections, given our edit model and language model.  In the OpenFst
library, the function that prunes an \fsm{} to contain only the best paths (i.e.,
lowest cost paths, in the Tropical Semiring) is called ShortestPath.  That operation
and terminology are exposed in the Kleene function \verb!$^shortestPath($fst, #num=1)!.  For
example, we can limit \verb!$corr! to the best five corrections using

\begin{Verbatim}
$corr = $^shortestPath($^lowerside(frm 
                                   _o_ 
                                   $editModel 
                                   _o_ 
                                   $langModel), 
                       5) ;
print $corr ;
fry : 7.0
from : 7.0
firm : 7.0
farm : 7.0
arm : 7.0
\end{Verbatim}

\noindent
Each of the five possible corrections has a weight of 7.0, indicating (in our
current model that assigns a weight of 7.0 to each change) that just one 
edit was needed to change \emph{frm} into the correctly
spelled word.  

If we ask ShortestPath to return just one result,

\begin{Verbatim}
$corr = $^shortestPath($^lowerside(frm 
                                   _o_ 
                                   $editModel 
                                   _o_ 
                                   $langModel), 
                       1) ;
print $corr ;
arm : 7.0
\end{Verbatim}

\noindent
it randomly gives us \emph{arm}, which is as close to \emph{frm} as the
other possibilities, given our edit model,
but intuitively is not the mostly likely correction.  

What's missing in this experiment so far is a modeling of the fact that some
words, in the language model, occur more frequently than others.  And when a
misspelled word is considered in isolation, possible corrections that are more
likely should have precedence over those that are less likely.  We
know intuitively that \emph{from}, a common English function word, occurs more often than the
alternatives \emph{farm}, \emph{firm}, \emph{fry} and \emph{arm},  and we can confirm this by looking at actual frequency counts.  Again, we will
model the likelihood of individual words in the language model using cost weights in the Tropical Semiring.


The Simpsons word list, in fact, gives us some additional information that allows
us to at least approximate the probability (and therefore the cost) of each
correct word.  The top of the list looks like this:

\begin{Verbatim}
Rank   Word                           Frequency
1      the                            (107946)
2      you                            (98068)
3      i                              (91502)
4      a                              (79241)
5      to                             (70362)
6      and                            (47916)
7      of                             (42175)
8      it                             (36497)
9      in                             (32503)
10     my                             (32254)
11     that                           (32083)
12     is                             (31533)
13     this                           (29902)
14     me                             (28208)
...
\end{Verbatim}

\noindent
The ``Frequency'' column shows the count of occurrences of each word from the
corpus.  For example, P(the), the probability of a word being \emph{the}, i.e., the probability of
selecting a word from the corpus at random, and having it be \emph{the}, is
computed as the count of occurrences of \emph{the}, here 107946, divided by the
total number of tokens in the corpus, i.e.\@

\begin{Verbatim}
	P(the) = 107946 / TotalTokens
\end{Verbatim}

\noindent
Disappointingly, the Simpsons wordlist doesn't tell us the TotalTokens,
so we are left to do some informed guessing.  In the Brown Corpus, the
most common word in English, 
\emph{the}, is said to account for almost 7\% of the tokens.  For now, let's accept
this number as being valid for the Simpsons corpus as well.  If 107946 instances of \emph{the}
represented 7\% of the corpus, then the total number of tokens would be a little over one and half
million.

\begin{Verbatim}
	107946 / TotalTokens = .07

	TotalTokens = 1542085
\end{Verbatim}

\noindent
For our example, let's use the round number of 1,500,000.  The probability of \emph{the}, and some
other common words, can then be estimated as

\begin{Verbatim}
	P(the) = 107946 / 1,500,000 = 0.07196

	P(you) = 98068 / 1,500000 = 0.06538

	P(i) = 91502 / 1,500,000 = 0.06100

	P(a) = 79241 / 1,500,000 = 0.05300
\end{Verbatim}

\noindent
Note that as the frequency of occurrence decreases, the probability value also decreases.  
As usual, if the probability of an event, such as the appearance of the word \emph{the} in
running English text, is p,
then the cost, C(the), is computed as $-$log(p).

\begin{Verbatim}
	C(the) = -log(107946 / 1,500,000) = 2.63159

	C(you) = -log(98068 / 1,500,000) = 2.72756

	C(i) = -log(91502 / 1,500,000) = 2.79686

	C(a) = -log(79241 / 1,500,000) = 2.94073
\end{Verbatim}

\noindent
Note that as the frequencies/probabilities decrease, the costs \emph{in}crease.
High probabilities correspond to low costs, and low probabilities correspond to
high costs.

These costs can now be included in an improved \emph{weighted} language model.  If we tediously precompute
the costs, we can simply list the cost for each word, using the usual
angle-bracket syntax.  Here is the Kleene syntax:

\begin{Verbatim}
$langModel = 
  the   <2.63159>
| you   <2.72756>
| i     <2.79686>
| a     <2.940731>
... ;
\end{Verbatim}

\noindent
Alternatively, we can let Kleene convert the probabilities into costs using the
\verb!#^prob2c()! function, ``probability to cost,'' which is pre-defined as 

\begin{Verbatim}
#^prob2c(#prob) {
    return -#^log(#prob) ;
}
\end{Verbatim}

\noindent
The definition of the improved, weighted language model might then look like this, with
\verb!#TotalTokens! defined as 1500000.0, a floating-point number, so that
when it is used in division, the result is also a float.

\begin{Verbatim}
#TotalTokens = 1500000.0 ;

$langModel = 
  the   <#^prob2c(107946 / #TotalTokens)>
| you   <#^prob2c(98068 / #TotalTokens)>
| i     <#^prob2c(91502 / #TotalTokens)>
| a     <#^prob2c(79241 / #TotalTokens)>
... ;
// and so on for all 5000 words in the model
\end{Verbatim}

\noindent
We could easily adjust the value of \verb!#TotalTokens! if we ever get more
precise information about the size of the corpus.

With the improved language model, now weighted, the best correction for \emph{frm} selected by
\verb!$^shortestPath()! is now \emph{from}, which intuitively seems right.

\begin{Verbatim}
$corr = $^shortestPath($^lowerside(frm 
                                   _o_ 
                                   $editModel 
                                   _o_ 
                                   $langModel)) ;
print $corr ;
from : 12.295898
\end{Verbatim}

\noindent
And we can see the \emph{n} best possible corrections by passing an optional numerical
argument to \verb!$^shortestPath()!, e.g.\@ 10:

\begin{Verbatim}
$corr = $^shortestPath($^lowerside(frm 
                                   _o_ 
                                   $editModel 
                                   _o_ 
                                   $langModel), 
                       10) ;
print $corr ;
from : 12.295898
from : 15.744141
arm : 15.818359
farm : 16.557617
fry : 17.483398
firm : 17.483398
for : 18.069336
i'm : 18.161133
are : 18.52832
him : 19.44336
\end{Verbatim}

\noindent
In the Tropical Semiring, as in golf, the lower scores are the better scores,
and \verb!$^shortestPath()! chooses the path(s) with the lowest cost(s).

After the weighted language model is defined, the rest of the final script is shown below,
including the definition of the convenience 
functions \verb!$^correct()!, which returns an \fsm{},
and \verb!^correctp()!, which simply prints out the results.  Both functions have an
optional numerical second argument, default 1, which controls how many paths are
retained in the shortest-path result, and an optional numerical third argument, 
default 7.0, which assigns the
weight for each editing change.

\begin{Verbatim}
$ident = . ;        // identity mapping, no change

//	Three kinds of change

$delete = .:"" ;    // map any symbol to the empty string

$insert = "":. ;    // map the empty string to any symbol

$swap = .:. - . ;   // map one symbol to another symbol 
                    // (not to itself)

// Higher change weights (costs) make changes more costly,
//  favoring corrections that look as much as possible
//  like the misspelled word. Lower change weights will
//  tend to favor "corrections" that are very high-frequency
//  words, no matter how high the edit distance between
//  the observed misspelled word and the proposed corrections.

$^editModel(#changecost) {
    return (  $ident <0.0> | 
              $delete <#changecost> | 
              $insert <#changecost> |
              $swap <#changecost>
           )* ;
}


$^correct($word, #num=1, #changecost = 7.0) {
    return $^shortestPath($^lowerside($word 
                                      _o_ 
                                      $^editModel(#changecost) 
                                      _o_
                                      $langModel), 
                          #num) ;
}

^correctp($word, #num=1, #changecost=7.0) {
    print "\n" ;
    print $^toString(#num) " correction(s) for " $word ;
    print $^shortestPath($^lowerside($word 
                                     _o_ 
                                    $^editModel(#changecost)
                                    _o_
                                    $langModel), 
                        #num) ;
}

// end of script
\end{Verbatim}

Once the script is loaded, one can simply call

\begin{alltt}
	print \verb!$^correct!(\emph{misspelledword}) ;
	print \verb!$^correct!(\emph{misspelledword}, 5) ;
	print \verb!$^correct!(\emph{misspelledword}, 5, 8.0) ;
\end{alltt}

\noindent
or just

\begin{alltt}
	\verb!^correctp!(\emph{misspelledword}) ;
	\verb!^correctp!(\emph{misspelledword}, 5) ;
	\verb!^correctp!(\emph{misspelledword}, 5, 8.0) ;
\end{alltt}

\noindent
The full script, \verb!weightededitdistance.kl!, is available from the
download page at \url{www.kleene-lang.org}.
